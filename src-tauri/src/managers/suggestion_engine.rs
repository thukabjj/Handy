//! Suggestion Engine Manager
//!
//! Provides real-time response suggestions during conversations.
//! Integrates keyword-based quick responses, RAG-powered suggestions,
//! and LLM-generated talking points.

use crate::managers::rag::RagManager;
use crate::ollama_client::OllamaClient;
use crate::settings::{QuickResponse, SuggestionsSettings, WarningSeverity};
use log::{debug, error, info, warn};
use serde::{Deserialize, Serialize};
use specta::Type;
use std::sync::Arc;
use std::time::Instant;
use tauri::{AppHandle, Emitter};
use tokio::sync::RwLock;

/// A suggestion generated by the engine
#[derive(Clone, Debug, Serialize, Deserialize, Type)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum Suggestion {
    /// Quick response triggered by keywords
    QuickResponse {
        id: String,
        text: String,
        confidence: f32,
        category: String,
        trigger_phrase: String,
    },
    /// Data point retrieved from knowledge base
    DataPoint {
        fact: String,
        source: String,
        relevance: f32,
    },
    /// AI-generated talking point
    TalkingPoint {
        point: String,
        rationale: String,
        confidence: f32,
    },
    /// Warning or alert about the conversation
    Warning {
        message: String,
        severity: WarningSeverity,
    },
}

/// Event payload for suggestions
#[derive(Clone, Debug, Serialize, Type)]
pub struct SuggestionsEvent {
    pub session_id: String,
    pub suggestions: Vec<Suggestion>,
    pub timestamp: i64,
}

/// Context for generating suggestions
#[derive(Clone, Debug)]
pub struct SuggestionContext {
    /// The current transcribed segment
    pub transcription: String,
    /// Previous context (recent transcriptions)
    pub previous_context: String,
    /// Session topic if set
    pub session_topic: Option<String>,
    /// Current session ID
    pub session_id: String,
}

/// Suggestion Engine
///
/// Generates real-time suggestions during Active Listening sessions by:
/// - Matching quick response templates against trigger phrases
/// - Querying the RAG knowledge base for relevant information
/// - Using the LLM to generate contextual talking points
pub struct SuggestionEngine {
    app_handle: AppHandle,
    /// Quick response templates
    quick_responses: Arc<RwLock<Vec<QuickResponse>>>,
    /// RAG manager for knowledge base queries
    rag_manager: Option<Arc<RagManager>>,
    /// Ollama client for LLM suggestions
    ollama_client: Arc<OllamaClient>,
    /// Current settings
    settings: Arc<RwLock<SuggestionsSettings>>,
}

impl SuggestionEngine {
    /// Create a new SuggestionEngine
    pub fn new(
        app_handle: &AppHandle,
        rag_manager: Option<Arc<RagManager>>,
        ollama_client: Arc<OllamaClient>,
        settings: SuggestionsSettings,
    ) -> Self {
        Self {
            app_handle: app_handle.clone(),
            quick_responses: Arc::new(RwLock::new(settings.quick_responses.clone())),
            rag_manager,
            ollama_client,
            settings: Arc::new(RwLock::new(settings)),
        }
    }

    /// Update settings
    pub async fn update_settings(&self, settings: SuggestionsSettings) {
        let mut current = self.settings.write().await;
        *self.quick_responses.write().await = settings.quick_responses.clone();
        *current = settings;
    }

    /// Check if suggestions are enabled
    pub async fn is_enabled(&self) -> bool {
        self.settings.read().await.enabled
    }

    /// Get suggestions for a given context
    pub async fn get_suggestions(&self, context: &SuggestionContext) -> Vec<Suggestion> {
        let settings = self.settings.read().await;

        if !settings.enabled {
            return vec![];
        }

        let mut suggestions = Vec::new();
        let start = Instant::now();

        // 1. Check quick response triggers
        let quick_suggestions = self.match_quick_responses(&context.transcription).await;
        suggestions.extend(quick_suggestions);

        // 2. Get RAG-based suggestions if enabled
        if settings.rag_suggestions_enabled {
            if let Some(rag_suggestions) = self.get_rag_suggestions(context).await {
                suggestions.extend(rag_suggestions);
            }
        }

        // 3. Get LLM-generated suggestions if enabled
        if settings.llm_suggestions_enabled && suggestions.len() < settings.max_suggestions {
            if let Some(llm_suggestions) = self.get_llm_suggestions(context).await {
                suggestions.extend(llm_suggestions);
            }
        }

        // Filter by confidence threshold
        suggestions.retain(|s| self.get_confidence(s) >= settings.min_confidence);

        // Sort by confidence and limit
        suggestions.sort_by(|a, b| {
            self.get_confidence(b)
                .partial_cmp(&self.get_confidence(a))
                .unwrap_or(std::cmp::Ordering::Equal)
        });
        suggestions.truncate(settings.max_suggestions);

        debug!(
            "Generated {} suggestions in {:?}",
            suggestions.len(),
            start.elapsed()
        );

        suggestions
    }

    /// Get the confidence score for a suggestion
    fn get_confidence(&self, suggestion: &Suggestion) -> f32 {
        match suggestion {
            Suggestion::QuickResponse { confidence, .. } => *confidence,
            Suggestion::DataPoint { relevance, .. } => *relevance,
            Suggestion::TalkingPoint { confidence, .. } => *confidence,
            Suggestion::Warning { severity, .. } => match severity {
                WarningSeverity::High => 1.0,
                WarningSeverity::Medium => 0.8,
                WarningSeverity::Low => 0.6,
            },
        }
    }

    /// Match quick response templates against the transcription
    async fn match_quick_responses(&self, transcription: &str) -> Vec<Suggestion> {
        let quick_responses = self.quick_responses.read().await;
        let transcription_lower = transcription.to_lowercase();
        let mut matches = Vec::new();

        for qr in quick_responses.iter() {
            if !qr.enabled {
                continue;
            }

            for trigger in &qr.trigger_phrases {
                let trigger_lower = trigger.to_lowercase();
                if transcription_lower.contains(&trigger_lower) {
                    // Calculate confidence based on how well the trigger matches
                    let confidence = self.calculate_trigger_confidence(&transcription_lower, &trigger_lower);

                    matches.push(Suggestion::QuickResponse {
                        id: qr.id.clone(),
                        text: qr.response_template.clone(),
                        confidence,
                        category: qr.category.clone(),
                        trigger_phrase: trigger.clone(),
                    });

                    // Only one match per quick response
                    break;
                }
            }
        }

        matches
    }

    /// Calculate confidence for a trigger match
    fn calculate_trigger_confidence(&self, text: &str, trigger: &str) -> f32 {
        // Base confidence for any match
        let mut confidence = 0.7;

        // Boost if trigger is a significant portion of the text
        let trigger_words: Vec<&str> = trigger.split_whitespace().collect();
        let text_words: Vec<&str> = text.split_whitespace().collect();

        if !text_words.is_empty() {
            let ratio = trigger_words.len() as f32 / text_words.len() as f32;
            confidence += ratio * 0.2;
        }

        // Boost for exact phrase match (not just contains)
        if text.split_whitespace().collect::<Vec<_>>().windows(trigger_words.len())
            .any(|window| window == trigger_words.as_slice())
        {
            confidence += 0.1;
        }

        confidence.min(1.0)
    }

    /// Get suggestions from the RAG knowledge base
    async fn get_rag_suggestions(&self, context: &SuggestionContext) -> Option<Vec<Suggestion>> {
        let rag = self.rag_manager.as_ref()?;

        // Query the knowledge base
        let query = format!(
            "{}\n\nContext: {}",
            context.transcription,
            context.previous_context
        );

        match rag.search(&query, 3).await {
            Ok(results) => {
                let suggestions: Vec<Suggestion> = results
                    .into_iter()
                    .filter(|r| r.similarity > 0.5) // Filter low relevance
                    .map(|r| Suggestion::DataPoint {
                        fact: r.chunk_text,
                        source: r.title.unwrap_or_else(|| "Knowledge Base".to_string()),
                        relevance: r.similarity,
                    })
                    .collect();

                if suggestions.is_empty() {
                    None
                } else {
                    Some(suggestions)
                }
            }
            Err(e) => {
                warn!("Failed to get RAG suggestions: {}", e);
                None
            }
        }
    }

    /// Get LLM-generated suggestions
    async fn get_llm_suggestions(&self, context: &SuggestionContext) -> Option<Vec<Suggestion>> {
        let app_settings = crate::settings::get_settings(&self.app_handle);
        let active_listening_settings = &app_settings.active_listening;

        // Build prompt for suggestion generation
        let prompt = format!(
            r#"You are a real-time conversation coach. Based on the following conversation segment, suggest 1-2 brief talking points or responses that would be helpful.

Current segment: {}
Previous context: {}
Topic: {}

Provide suggestions in this format:
POINT: [Your suggested talking point]
RATIONALE: [Brief reason why this is helpful]

Be concise - each point should be actionable and under 50 words. Only suggest if truly relevant."#,
            context.transcription,
            context.previous_context,
            context.session_topic.as_deref().unwrap_or("General conversation")
        );

        // Use Ollama to generate suggestions
        match self
            .ollama_client
            .generate(
                &active_listening_settings.ollama_model,
                prompt,
            )
            .await
        {
            Ok(response) => {
                let suggestions = self.parse_llm_suggestions(&response);
                if suggestions.is_empty() {
                    None
                } else {
                    Some(suggestions)
                }
            }
            Err(e) => {
                warn!("Failed to get LLM suggestions: {}", e);
                None
            }
        }
    }

    /// Parse LLM response into suggestions
    fn parse_llm_suggestions(&self, response: &str) -> Vec<Suggestion> {
        let mut suggestions = Vec::new();
        let lines: Vec<&str> = response.lines().collect();

        let mut current_point: Option<String> = None;
        let mut current_rationale: Option<String> = None;

        for line in lines {
            let line = line.trim();

            if line.starts_with("POINT:") {
                // Save previous point if exists
                if let Some(point) = current_point.take() {
                    suggestions.push(Suggestion::TalkingPoint {
                        point,
                        rationale: current_rationale.take().unwrap_or_default(),
                        confidence: 0.7,
                    });
                }
                current_point = Some(line.trim_start_matches("POINT:").trim().to_string());
            } else if line.starts_with("RATIONALE:") {
                current_rationale = Some(line.trim_start_matches("RATIONALE:").trim().to_string());
            }
        }

        // Don't forget the last point
        if let Some(point) = current_point {
            suggestions.push(Suggestion::TalkingPoint {
                point,
                rationale: current_rationale.unwrap_or_default(),
                confidence: 0.7,
            });
        }

        suggestions
    }

    /// Emit suggestions event to the frontend
    pub async fn emit_suggestions(&self, session_id: &str, suggestions: Vec<Suggestion>) {
        let event = SuggestionsEvent {
            session_id: session_id.to_string(),
            suggestions,
            timestamp: chrono::Utc::now().timestamp_millis(),
        };

        if let Err(e) = self.app_handle.emit("suggestions", &event) {
            error!("Failed to emit suggestions event: {}", e);
        }
    }

    /// Get all quick responses
    pub async fn get_quick_responses(&self) -> Vec<QuickResponse> {
        self.quick_responses.read().await.clone()
    }

    /// Get quick responses by category
    pub async fn get_quick_responses_by_category(&self, category: &str) -> Vec<QuickResponse> {
        self.quick_responses
            .read()
            .await
            .iter()
            .filter(|qr| qr.category == category && qr.enabled)
            .cloned()
            .collect()
    }

    /// Add a new quick response
    pub async fn add_quick_response(&self, response: QuickResponse) {
        self.quick_responses.write().await.push(response.clone());

        // Also update in settings
        let mut settings = self.settings.write().await;
        settings.quick_responses.push(response);
    }

    /// Update a quick response
    pub async fn update_quick_response(&self, response: QuickResponse) -> bool {
        let mut quick_responses = self.quick_responses.write().await;

        if let Some(existing) = quick_responses.iter_mut().find(|qr| qr.id == response.id) {
            *existing = response.clone();

            // Also update in settings
            let mut settings = self.settings.write().await;
            if let Some(settings_qr) = settings.quick_responses.iter_mut().find(|qr| qr.id == response.id) {
                *settings_qr = response;
            }

            true
        } else {
            false
        }
    }

    /// Delete a quick response
    pub async fn delete_quick_response(&self, id: &str) -> bool {
        let mut quick_responses = self.quick_responses.write().await;
        let initial_len = quick_responses.len();
        quick_responses.retain(|qr| qr.id != id);

        if quick_responses.len() < initial_len {
            // Also remove from settings
            let mut settings = self.settings.write().await;
            settings.quick_responses.retain(|qr| qr.id != id);
            true
        } else {
            false
        }
    }

    /// Toggle a quick response's enabled state
    pub async fn toggle_quick_response(&self, id: &str) -> Option<bool> {
        let mut quick_responses = self.quick_responses.write().await;

        if let Some(qr) = quick_responses.iter_mut().find(|qr| qr.id == id) {
            qr.enabled = !qr.enabled;
            let new_state = qr.enabled;

            // Also update in settings
            let mut settings = self.settings.write().await;
            if let Some(settings_qr) = settings.quick_responses.iter_mut().find(|qr| qr.id == id) {
                settings_qr.enabled = new_state;
            }

            Some(new_state)
        } else {
            None
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_calculate_trigger_confidence_basic() {
        // Create a minimal engine for testing
        // Note: In actual tests, we'd use a proper mock
        let text = "the product is too expensive for our budget";
        let trigger = "too expensive";

        // Trigger is contained in text, so confidence should be > 0.7
        assert!(text.to_lowercase().contains(&trigger.to_lowercase()));
    }

    #[test]
    fn test_parse_llm_suggestions() {
        let response = r#"POINT: Consider addressing the budget concerns directly
RATIONALE: The customer mentioned budget constraints twice

POINT: Offer a payment plan option
RATIONALE: This can help overcome price objections"#;

        // Test that we can parse multiple suggestions
        let lines: Vec<&str> = response.lines().collect();
        let point_count = lines.iter().filter(|l| l.starts_with("POINT:")).count();
        assert_eq!(point_count, 2);
    }

    #[test]
    fn test_suggestion_confidence_ordering() {
        let suggestions = vec![
            Suggestion::QuickResponse {
                id: "1".to_string(),
                text: "Low".to_string(),
                confidence: 0.5,
                category: "test".to_string(),
                trigger_phrase: "test".to_string(),
            },
            Suggestion::QuickResponse {
                id: "2".to_string(),
                text: "High".to_string(),
                confidence: 0.9,
                category: "test".to_string(),
                trigger_phrase: "test".to_string(),
            },
            Suggestion::QuickResponse {
                id: "3".to_string(),
                text: "Medium".to_string(),
                confidence: 0.7,
                category: "test".to_string(),
                trigger_phrase: "test".to_string(),
            },
        ];

        // Verify we can sort by confidence
        let mut sorted = suggestions.clone();
        sorted.sort_by(|a, b| {
            let conf_a = match a {
                Suggestion::QuickResponse { confidence, .. } => *confidence,
                _ => 0.0,
            };
            let conf_b = match b {
                Suggestion::QuickResponse { confidence, .. } => *confidence,
                _ => 0.0,
            };
            conf_b.partial_cmp(&conf_a).unwrap()
        });

        // First should be highest confidence
        if let Suggestion::QuickResponse { confidence, .. } = &sorted[0] {
            assert_eq!(*confidence, 0.9);
        }
    }

    #[test]
    fn test_warning_severity_confidence() {
        // High severity warnings should have high confidence
        let high = WarningSeverity::High;
        let medium = WarningSeverity::Medium;
        let low = WarningSeverity::Low;

        // Just verify the enum values exist and can be compared
        assert_ne!(high, medium);
        assert_ne!(medium, low);
    }
}
